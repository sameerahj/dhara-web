<!DOCTYPE html>
<h4>What are workflows?</h4>

<p>A workflow is an abstract model of a series of steps connected to represent a real world process where each
    step defines a specific task or functionality. According to [2], ìA program (or script) is to a workflow what
    an unstructured document is to a (structured) databaseî. Workflows are capable of hiding the complexity of the
    execution process. They provide a representation of complex analysis composed of diverse models [3]. A workflow
    is typically authored using a visual front-end or can be hard-coded, and their execution is delegated to a
    workflow execution engine that handles the invocation of the remote applications [4]. Workflow systems can be
    categorized mainly into two sections; business workflows, scientific workflows.</p>

<h3>Business workflows vs. scientific workflows</h3>

<p>Example use cases of scientific workflow: [1]</p>
<ul>
    <li>In astronomy, scientists are using workflows to generate science-grade mosaics of the sky, to examine the structure
        of galaxies, and, in general, to understand the structure of the universe.</li>
    <li>In bioinformatics, researchers are using workflows to understand the underpinnings of complex diseases.</li>
    <li>In physics, workflows are used to search for gravitational waves and model the structure of atoms.</li>
    <li>In ecology, scientists use workflows to explore the issues of biodiversity.</li>
</ul>
<p>
    One of the major differences between business and scientific workflows are that, business workflows are control
    flow oriented while scientific processes are data flow oriented. This fact can be explained as follows: a dependency
    in scientific flow "characterizes the data relationship between an input and output of two respective tasks of a scientific
    workflow whereas a dependency characterizes the partial order relation between the executions of two tasks in a business
    workflow."[1]The data flow does not impose an order of execution, it only specifies the input and output of components;
    component B should use component A‚Äôs output as input. In scientific workflows a number of tasks can be concurrent but
    not in business workflows. Tasks represented in parallel will mean a conditional choice of one of them.
</p>

<p>Business workflows follow a clear set of rules but scientific workflows involve several different complex computing
    processes and complex data. "To design a robust, fault-tolerance, and adaptive workflow, scientific workflow requires
    modeling of complex control-flow in an essential data-flow oriented environment".[1] Another difference is that scientists
    are concerned about the intermediate steps, results and data of a scientific process while business people are only
    interested in optimizing possible parts of their process to reduce maintenance cost. Therefore "provenance management"
    is identified to be of significant importance in scientific workflow management systems. [1] Scientific workflows
    usually have simple computational models but business workflows have expressive languages (BPEL) to specify complex
    control flow. Scientific workflows are usually based on data intensive tasks. [3] A point related to the above is;
    "Scientific research requires flexible design and exploration capabilities that appear to depart significantly from
    the more prescriptive use of workflows in business" [3]. "Another distinctive issue of scientific workflows is the
    variety and heterogeneity of data within a single workflow."[3]</p>

<h3>Scientific workflows</h3>

<p> We are focused on scientific workflows which are significantly different from the business workflows.
    One of the major differences between business and scientific workflows are that, business workflows are
    control flow oriented while scientific processes are data flow oriented.
    The data flow does not impose an order of execution, it only specifies the input and output of components;
    component B should use component A?s output as input. Three motivations for scientific workflow have been
    identified as follows: [5]
    </p>
<ul>
    <li>Some complex e-science applications often require the creation of a collaborative workflow</li>
    <li>Many e-scientists lack the necessary low-level expertise to utilize the current generation of
        underlying computing infrastructure such as Grid toolkits</li>
    <li>Workflow specifications can be reused, modified and shared once they are defined.</li>
</ul>

<p>With the availability of vast computational power, scientific communities are engaged in solving
    interconnected problems spread over multiple disciplines. These activities have accelerated by
    mapping them onto a workflow. For an example, in earthquake science, workflows are used to predict
    the magnitude of earthquakes within a geographic area over a period of time. Scientific workflow is
    the procedure of combining data and processes into a structured set of steps which can operate as a
    solution to a scientific problem. It includes declarative description about each component and its input
    and output. [2] Workflows can be generated manually by scientists or using third party tools to assist larger
    workflows. They utilize distributed resources in order to access, manage and process large amounts of data
    from a higher-level [6]. Processing and managing such large amounts of data require addressing proper techniques
    for storage facilities and to handle scaling resources.
</p>

<h3>Scientific workflow management systems</h3>
<p>
    Both scientific and business workflow management systems provide means to, [1]</p>
<ol>
    <li>Model and specify processes with design primitives</li>
    <li>Re-engineer developed processes such as verification and optimization</li>
    <li>Automating the execution of processes by scheduling, controlling and monitoring the tasks</li>
</ol>
<p>
    Scientific workflow management systems act as a middleware for creating, combining, executing workflows and data
    management tools. They have evolved with the aim of producing a high level platform enabling the end-user scientists
    to create, manage, compose, execute and test scientific workflows without concentrating on the low-level computational
    technology. In the paper [2] scientific workflow management systems are recognized as providing means to; model and
    specify processes with design primitives, re-engineer developed processes such as verification and optimization,
    automating the execution of processes by scheduling, controlling and monitoring the tasks.    </p>
   <p>Before the invention of workflow management systems, programs and scripts were used as means to run scientific
    process with predefined steps. But workflow management systems provide advantages over the earlier mechanism for
    constructing and managing computational tasks. Main advantage is that they provide a simple programming model to
    compose a sequence of tasks simply by connecting the outputs of one task to the inputs of another. An added advantage
    is the intuitive visual programming interfaces of workflow management systems, which make them more suitable for
    users without ample programming expertise. [7]     </p>
   <p> Currently there are many workflow management systems developed targeting various scientific communities such as
    Triana, Pegusas, Taverna and Kepler [8]. Most of these systems are designed to solve problems in certain domains.</p>

<h3>Desirable application level features of scientific workflow management systems</h3>

<h5>Handling dynamic workflows</h5>
<p>According the nature of experiments, the vision of supporting dynamic, adaptive workflows is accelerated.
    Capturing mechanisms to create certain results via reproducibility is one aspect of handling dynamic workflows [33].
    The demand on dynamic workflows increases due to the nature of scientific methodologies, such as; the runtime
    decisions on later steps of a workflow may need the initial steps? results. Dynamic workflows may respond for
    external events [33] and may depend on the results of data analysis computation. In addition, dynamic workflows
    could have taken place due to observation and modification of different scientists and researchers in same
    experimental procedure. In contrast the workflow execution may determine its path dynamically at runtime.</p>
     <p>Thus managing a dynamic workflow is a challenge in the application. The managing process of a workflow is
         evolving through cycles. Workflow can be share, refine and rerun to check the result. The process will
         continue until scientists get a satisfied result. Hence a proper user interaction should handle via an
         appropriate user interface and the results of execution should be query and display in an understandable
         manner. Real time workflow status monitoring and dynamic notification is also plays a key role in the dynamic
         workflow handling.</p>

<h5>Interoperability</h5>
<p>Emerging of several workflow management systems with specific features has stimulated the scientific researches in
    recent past. The collaborative nature in scientific researches results multiple contributors from geographically
    distributed locations taking part in developing a single workflow experiment. Almost all of the widely used workflow
    systems have been developed with different communities, targeting different domains and exhibiting specific features
    within relevant area. They use different workflow engines, description languages, and formalism which makes it
    less interoperable [34]. Due to that differentiation it is hard to express workflow of one system using a description
    language of another [34].</p>
    <p>Thus a key architectural requirement in SWfMS is to facilitate the interoperability between different SWfMS,
        so that one system can take benefits of tools and unique features of another system [35]. This enables the
        reusing and sharing of workflows among systems. Accordingly in order to achieve interconnection within different
        systems, workflows should be interoperable.</p>

     <p>The interoperability can be achieved in 3 levels [35];</p>
        <ol>
            <li>Task-level</li>
            <li>Workflow-level</li>
            <li>Subsystem-level</li>
        </ol>
     <p>GEMLCA service [34] has achieved interoperability of heterogeneous workflow systems via workflow engines
         integration which can be listed under the 3rd level achievement. The proposed solution executes workflow in
         the native platform and exposes it to run in non-native platform. The basic idea in the integration of workflows
         is the ability to use a workflow as a one node/task in the system.</p>

<h5>Data management</h5>
<p>"Individual scientists should be able to steer the system to conduct unique analyses and create novel workflows
    with previously unseen combinations and configurations of models". [2]. These flexible environments must ensure security,
    reliability and scalability as well.</p>

<h5>Data management</h5>
<p>Data management is a one of major requirement in the workflow lifecycle. Data management includes data inputs
    in creation and execution workflows with handling results. These steps access terabytes of data at initials
    steps, intermediate levels and in end results as well. Data produced and used in the workflow execution process
    deals with several data types, such as provenance data for collaboration purposes, metadata to query or store
    workflow information, input data from remote data sources, data from intermediate levels to be analyzed and
    feed the later tasks [34].</p>
 <p>There are several procedures that need to be following in data handling. Metadata or provenance data need to agree
     to a community based standard which is rich enough to describe the data sets and information on data [34]. In
     addition maintaining metadata catalogues to query and store metadata is another requirement. Use of common metadata
     catalog independently from the systems will provide good coordination of data access and security.</p>
 <p>In contrast identifying the input data formats in various remote processes for the execution of workflows is
     a major concern. Schedulers in the system responsible for selecting data sets and appropriate computational
     resources to run tasks [34].Required data need to feed consistently and fast to places where computation takes
     place. Thus a proper data management section is a major requirement in a workflow management system, specially
     using for data-intensive applications.</p>

 <h5>Quality of Service</h5>
 <p>Requirements of Quality of Service (QoS) in workflow management systems need to be specified and optimized.
     There are several concerns that took into account in recent past such as; reducing execution time, maximizing
     bandwidth etc. [33]. In addition to time constraints there are some other QoS parameters; responsiveness, fault
     tolerance, security, and costs [33]. Performance, reliability, security and fault tolerance are also related to
     QoS. The WfMS need to meet the QoS requirements via exposing them to demanding techniques. Accessing cloud
     services opposed to traditional reservation of resources is one aspect of optimizing resource allocation with
     dynamic scaling up and down requirements.</p>
<h5>Ease of use</h5>
<p>Scientific workflow execution requires high computational power, which is gained through cyber infrastructure
    such as Grid computing and Cloud computing. Open Science Grid and TeraGrid are science gateways which provide
    widely used and well tested computational capabilities and services [33]. Nevertheless effective and optimized
    usage of these computing resources requires expertise of tools such as Grid Toolkits. For the average scientist
    or researcher who does not have ample expertise in such tools, effective utilization of the underlying computational
    infrastructure becomes a tedious task.</p>
<p>Computational complexities in scientific domain require undivided focus of the scientist. Therefore it is a key
    requirement of scientific workflow management systems to provide its users with optimum level ease of use for
    handling underlying resources. Separating the science-focused and technology-independent problem solving environment
    from the underlying advanced computing infrastructure is considered as the potential approach for this issue in
    the paper [35].</p>

<h5>Provenance tracking</h5>
<p>Provenance tacking has become the main focus in many research projects as it is a critical component in workflow
    sharing, reusing and end result reproducibility. Provenance provides answers to: ìWho created this data product
    and when? When was it modified and by whom? What was the process used to create the data product? Were two data
    products derived from the same raw data?î [7]. Scientists and researchers often require putting in substantial
    effort for managing the large amount of provenance information related to their work. Therefore scientific workflow
    management systems should provide automated capabilities to capture metadata, log the sequence in applied steps,
    parameter settings and intermediate data products [36].</p>
<p>Sharing and reusing of workflows are common practices in scientific communities. Tracking and efficient capturing
    of provenance information provides important information that is the key to preserving data, determining the data
    quality and authorship [7]. Reproducibility is an essential feature of computational scientific experiments as same
    as conventional laboratory experiments [36]. When provenance information is effectively embedded in workflows fellow
    scientists can redo a particular experiment using the same data, following the same steps, evaluating the intermediate
    data products and finally reproducing the experiment results. Holistically provenance information enables the
    scientists to reproduce experiment results and evaluate the validity of each other?s hypotheses [33].</p>

<h5>Sharing and reuse</h5>
<p>Sharing and reuse in various aspects is widely used practice within the scientific communities. A Workflow is a
    great way to electronically capture a process, which paves the way to sharing and reusing them [33] .
    Encouraging researchers to include workflow usage into their practices would contribute to rapid advancements.
    Nevertheless there are several workflow management systems in use at present. Therefore it is essential that reuse
    is supported among them. Support for reuse has to be done in two different aspects: workflow semantics and
    infrastructure [33].</p>
<p>Researchers have to agree on process semantics used in workflows. Workflow management systems (WfMS) should provide
    capabilities and emphasis on constructing workflows in formal and explicit ways. Sharing is the major intention
    behind workflow reuse. Nonetheless reusing is often followed by refinements. Abstractions allow scientists to
    identify the level of description useful to share workflows so that other scientists could refine and use [33].
    Hence abstraction and refinement capabilities require being present in workflow management systems.</p>
<p>There is a range of distinct computational environments such as Open Science Grid, TeraGrid and Amazon EC2, which
    are used to run workflows. As a result workflow sharing and reuse often lead to instances where the same workflow
    requires to be run on different and heterogeneous environments. Workflow management systems should allow user
    created workflows to be easily run in these different environments without alteration.</p>

<h5>Monitoring and Error handling</h5>
<p>Long running and data intensive scientific workflows are common scenarios. These workflows are often collaboratively
    developed and even modified. Then these workflows are constructed with various distributed tasks over network
    communications [35] . All in all above characteristics of scientific workflows impose additional challenges regarding
    errors and failures. Therefore scientific workflow management systems require the capability to monitor status and
    failure of a running workflows in various levels including mechanisms for catching and handling errors automatically [35].</p>
<p>Pegasus workflow management system provides error handling which has been identified as a key feature of it.
    It tries error handling in various levels: retrying tasks, retrying entire workflow, providing workflow-level
    check-pointing, re-mapping portions of the workflow, trying alternative data sources and providing a rescue workflow
    containing a description of only the work remaining as a last resort [30].</p>

<h4>References</h4>
<ul class="ulChange"    >
    <li>[1] U. Yildiz, A. Guabtni, and A. H. H. Ngu, "Business versus ScientiÔ¨Åc WorkÔ¨Çow A Comparative study.pdf."</li>
    <li>[2] Y. Gil, M. Ellisman, T. Fahringer, G. Fox, D. Gannon, C. Goble, and J. Myers, "Examining the scientific workflows," 2007.</li>
    <li>[3] S. B. Davidson and J. Freire, "Provenance and ScientiÔ¨Åc WorkÔ¨Çows.pdf."</li>
    <li>[4] J. Chen, "On Scientific Workflow."</li>
    <li>[5] S. Bharathi, A. Chervenak, E. Deelman, GaurangMehta, M.-H. Su, and K. Vahi, "Characterization of ScientiÔ¨Åc WorkÔ¨Çows.pdf."</li>
</ul>